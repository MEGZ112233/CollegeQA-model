# -*- coding: utf-8 -*-
"""dataSetGenerationFinalExams.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yR1N_53nqRQvRX--aSv4iaMCtZzcqSHM
"""

# !pip install langchain langchain-community langchain-google-genai huggingface-hub sentence-transformers python-dotenv ollama
# !pip install faiss-cpu

from langchain_community.llms import Ollama
from langchain_community.embeddings import HuggingFaceEmbeddings
import os
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain.schema import StrOutputParser
import time

models = [
    ChatGoogleGenerativeAI(model="gemini-2.0-flash-thinking-exp-01-21", api_key="your_api_key_1"),
    ChatGoogleGenerativeAI(model="gemini-2.0-flash-thinking-exp-01-21", api_key="your_api_key_2"),
    ChatGoogleGenerativeAI(model="gemini-2.0-flash-thinking-exp-01-21", api_key="your_api_key_3"),
    ChatGoogleGenerativeAI(model="gemini-2.0-flash-thinking-exp-01-21", api_key="your_api_key_4"),
    ChatGoogleGenerativeAI(model="gemini-2.0-flash-thinking-exp-01-21", api_key="your_api_key_5"),
]

embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-large-en-v1.5",  # or choose another BGE model variant
    model_kwargs={'device': 'cpu'}
)



with open("/content/FinalExams.txt", "r") as file:
    chunks = file.readlines()  # Reads all lines into a list
    chunks = [line.strip() for line in chunks]  # Removes trailing newline characters
chunks = [line for line in chunks if line]
print(chunks)

from langchain.vectorstores import FAISS
faiss_index = FAISS.from_texts(chunks, embeddings)

chunk_dict = {chunks[i] : i  for i in range(len(chunks)) }

from langchain.prompts import PromptTemplate

template = """
     You are an AI assistant that generates multiple variations of a question based on a given text context.
    - Your task is to rephrase the same question in at least **5 different ways** while maintaining its original meaning.
    - Include some variations that use common abbreviations (such as "algo" for "algorithm", "DS" for "data structures", "calc" for "calculus", etc.) if available .
    - Only output the questions.
    - Each question should be on a new line.
    - Do not include any explanations, headings, or additional text.

    ## Example Context:
    "The final exam for the Introduction to Algorithms course (CS101), offered by the Computer Science department, from 2024, is available at the following link:
    [https://drive.google.com/file/d/1xgNmx5X4Bj7cQNTb8ma4STZ-WgCHm05L/view?usp=sharing]."

    ### Expected Output:
    Where can I find the final exam link for the Introduction to Algorithms course from 2024?
    Could you share the link to the 2024 CS101 final exam?
    Is there a link available for the 2024 Algo course exam?
    How can I access the final exam for the Intro to Algorithms class from 2024?
    Can you provide the exam link for the CS101 Algo course from 2024?
    Where's the link for the 2024 Algorithms final?

    ## Now, generate multiple reworded questions for the following context:
    {context}
"""

prompt = PromptTemplate(
    template=template,
    input_variables=["context"]
)
print(prompt.format(context=chunks[0]))

idx = 0
def get_query (context) :
    while (True) :
      global idx
      idx+=1
      try :
        chain  = prompt |  models[idx%5] | StrOutputParser()
        response  = chain.invoke({"context": context})
        return response
      except Exception as e  :
        print(e)
        time.sleep(10)

result  =  get_query(chunks[17])
print(result)

import csv

csv_filename = "/content/dataset.csv"

with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:
    writer = csv.writer(file)
    writer.writerow(["Question", "chunk"])

def save_to_csv(data , context, filename="/content/dataset.csv"):
       x = [line.strip() for line in data.split('\n')]
       try :
             with open(filename, 'a', newline='', encoding='utf-8') as file:
                writer = csv.writer(file)
                for i in range(len(x)):
                    question = x[i]
                    writer.writerow([question, context])
             print(f"Data successfully saved to {filename}")
       except Exception as e :
            print("wrong in saving chunk ")

for i in range(len(chunks)):
  result  =  get_query(chunks[i])
  save_to_csv(result , chunks[i])

import pandas as pd

df = pd.read_csv("/content/dataset.csv")

RequiredNumber =  1000
def eval_MRR ():
    mrr = 0
    for i in range(RequiredNumber):
        RetreivedChunks = faiss_index.similarity_search(df["Question"][i] , k  = 5)
        for j in range(5):
            if (RetreivedChunks[j].page_content== df["chunk"][i]):
                mrr += 1/(j+1)
                break

    return mrr/RequiredNumber

ass =  eval_MRR()
print(ass)

RetreivedChunks = faiss_index.similarity_search("i need to study OR course final exams" , k  = 35)

for i in range(35):
  print(RetreivedChunks[i].page_content)

print(len(chunks))