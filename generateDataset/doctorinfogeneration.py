# -*- coding: utf-8 -*-
"""DoctorInfoGeneration.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TCzvuDz7RgnzWd3lC1NSYOnmRbfnY3RS
"""



from langchain_community.llms import Ollama
from langchain_community.embeddings import HuggingFaceEmbeddings
import os
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain.schema import StrOutputParser
import time

models = [
    ChatGoogleGenerativeAI(model="gemini-2.0-flash", api_key="AIzaSyAm6iLxJOey5xUM4eNN0UgaZbKytIlnMJg"),
    ChatGoogleGenerativeAI(model="gemini-2.0-flash", api_key="AIzaSyB0kYyqJ_HEaxRODzg8LaVz85KJs6oS7Dk"),
    ChatGoogleGenerativeAI(model="gemini-2.0-flash", api_key="AIzaSyCHigx1CQfHtl-66zuM1KSisHqfgXpxO48"),
    ChatGoogleGenerativeAI(model="gemini-2.0-flash", api_key="AIzaSyAVaS0j7g2o2ur665K_SldrwYoritqU5Mg"),
    ChatGoogleGenerativeAI(model="gemini-2.0-flash", api_key="AIzaSyBOeuFxdaQIwFqUCDR_ALr_MPLVTgF_7bI"),
]

embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-large-en-v1.5",  # or choose another BGE model variant
    model_kwargs={'device': 'cpu'}
)



with open("/content/DoctorInfo.txt", "r") as file:
    chunks = file.readlines()  # Reads all lines into a list
    chunks = [line.strip() for line in chunks]  # Removes trailing newline characters

print(chunks)



from langchain.vectorstores import FAISS
faiss_index = FAISS.from_texts(chunks, embeddings)

chunk_dict = {chunks[i] : i  for i in range(len(chunks)) }

from langchain.prompts import PromptTemplate

template = """
  You are an AI assistant that generates questions based on given text contexts.
    Your task is to generate exactly **two questions** for each provided context.

    - Only output the questions.
    - Each question should be on a new line.
    - Do not include any explanations, headings, or additional text.

    ## Example Context:
    "Dr. Abeer Mahmoud is part of the CS department and can be reached at abeer.mahmoud@cis.asu.edu.eg."

    ### Expected Output:
    Which department does Dr. Abeer Mahmoud belong to?
    What is Dr. Abeer Mahmoud's email address?

    ## Now, generate questions for the following contexts:
    {context}
"""

prompt = PromptTemplate(
    template=template,
    input_variables=["context"]
)
print(prompt.format(context=chunks[0]))

idx = 0
def get_query (context) :
    while (True) :
      global idx
      idx+=1
      try :
        chain  = prompt |  models[idx%5] | StrOutputParser()
        response  = chain.invoke({"context": context})
        return response
      except Exception as e  :
        print(e)
        time.sleep(10)

result  =  get_query(chunks[0])
print(result)

import csv

csv_filename = "/content/dataset.csv"

with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:
    writer = csv.writer(file)
    writer.writerow(["Question", "chunk Index"])

def save_to_csv(data , context_index, filename="/content/dataset.csv"):
       x = [line.strip() for line in data.split('\n')]
       try :
             with open(filename, 'a', newline='', encoding='utf-8') as file:
                writer = csv.writer(file)
                for i in range(len(x)):
                    question = x[i]
                    writer.writerow([question, context_index])
             print(f"Data successfully saved to {filename}")
       except Exception as e :
            print("wrong in saving chunk ")

for i in range(len(chunks)):
  result  =  get_query(chunks[i])
  save_to_csv(result , i)

import pandas as pd

df = pd.read_csv("/content/dataset.csv")

RequiredNumber =  186 * 2
def eval_MRR ():
    mrr = 0
    for i in range(RequiredNumber):
        RetreivedChunks = faiss_index.similarity_search(df["Question"][i] , k  = 5)
        for j in range(5):
            if (chunk_dict[RetreivedChunks[j].page_content] == df["chunk Index"][i]):
                mrr += 1/(j+1)
                break

    return mrr/RequiredNumber

ass =  eval_MRR()
print(ass)

RetreivedChunks = faiss_index.similarity_search("what is the email address of Dr. mohamed essam  ? " , k  = 5)

print(RetreivedChunks)