# -*- coding: utf-8 -*-
"""GenerateAnouncementDataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1STWoVDMy4Erec84WD2HPh-KNf-D-d-BF
"""

# !pip install langchain langchain-community langchain-google-genai huggingface-hub sentence-transformers python-dotenv ollama faiss-cpu

from langchain_community.llms import Ollama
from langchain_community.embeddings import HuggingFaceEmbeddings
import os
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain.schema import StrOutputParser
import time

models = [
    ChatGoogleGenerativeAI(model="gemini-2.0-flash-thinking-exp-01-21", api_key="your_api_key_1"),
    ChatGoogleGenerativeAI(model="gemini-2.0-flash-thinking-exp-01-21", api_key="your_api_key_2"),
    ChatGoogleGenerativeAI(model="gemini-2.0-flash-thinking-exp-01-21", api_key="your_api_key_3"),
    ChatGoogleGenerativeAI(model="gemini-2.0-flash-thinking-exp-01-21", api_key="your_api_key_4"),
    ChatGoogleGenerativeAI(model="gemini-2.0-flash-thinking-exp-01-21", api_key="your_api_key_5"),
]

embeddings = HuggingFaceEmbeddings(
    model_name="Bo8dady/finetuned-College-embeddings",  # or choose another BGE model variant
    model_kwargs={'device': 'cpu'}
)

with open("/content/Announcemnt_data.txt", "r") as file:
    chunks = file.readlines()  # Reads all lines into a list
    chunks = [line.strip() for line in chunks]  # Removes trailing newline characters
chunks = [line for line in chunks if line]
print(chunks)

from langchain.vectorstores import FAISS
faiss_index = FAISS.from_texts(chunks, embeddings)

from langchain.prompts import PromptTemplate

template = """
     You are an advanced AI tasked with generating multiple structured questions from educational announcements.

Given the following announcement: {context}

Generate 4-5 distinct question variations that comprehensively explore the announcement's key details. Follow these guidelines:
1. Use different interrogative words (When, What, Where, Who, How)
2. Vary question structures and phrasings
3. Capture the core information from the original text
4. Ensure questions are grammatically correct and meaningful
5. Provide unique perspectives on the announcement

Output Requirements:
- Generate exactly 4-7 questions
- Place each question on a separate line
- Avoid redundant or overly similar questions

Questions:
"""

prompt = PromptTemplate(
    template=template,
    input_variables=["context"]
)
print(prompt.format(context=chunks[0]))

idx = 0
def get_query (context) :
    while (True) :
      global idx
      idx+=1
      try :
        chain  = prompt |  models[idx%len(models)] | StrOutputParser()
        response  = chain.invoke({"context": context})
        return response
      except Exception as e  :
        print(e)
        time.sleep(10)

result  =  get_query(chunks[17])
print(result)

import csv

csv_filename = "/content/dataset.csv"

with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:
    writer = csv.writer(file)
    writer.writerow(["Question", "chunk"])

def save_to_csv(data , context, filename="/content/dataset.csv"):
       x = [line.strip() for line in data.split('\n')]
       try :
             with open(filename, 'a', newline='', encoding='utf-8') as file:
                writer = csv.writer(file)
                for i in range(len(x)):
                    question = x[i]
                    writer.writerow([question, context])
             print(f"Data successfully saved to {filename}")
       except Exception as e :
            print("wrong in saving chunk ")

for i in range(len(chunks)):
  result  =  get_query(chunks[i])
  save_to_csv(result , chunks[i])