This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: preprocessData/, chunks.csv, merged.csv, data/
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
CollegeChatBot/
  app.py
  chains_and_prompts.py
  chatbot.py
  config.py
  faiss_handler.py
  models.py
  splitData.py
  utils.py
generateDataset/
  datasetgenerationfinalexams.py
  doctorinfogeneration.py
  generateanouncementdataset.py
  rulesgeneration_dataset.py
ModularChatBot.py
requirements.txt
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="CollegeChatBot/config.py">
import os
from dotenv import load_dotenv

def setup_environment():
    load_dotenv()

def get_api_key(index):
    return os.getenv(f"G{index}")
</file>

<file path="CollegeChatBot/faiss_handler.py">
import os
from langchain.vectorstores import FAISS

def manage_faiss_index(chunks, embeddings, index_folder):
    index_file = os.path.join(index_folder, "index.faiss")
    if os.path.exists(index_file):
        try:
            return FAISS.load_local(index_folder, embeddings, allow_dangerous_deserialization=True)
        except Exception:
            pass
    index = FAISS.from_texts(chunks, embeddings)
    index.save_local(index_folder)
    return index
</file>

<file path="CollegeChatBot/models.py">
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
import os
from config import get_api_key

def initialize_models(model_name, api_len=10):
    embedding_model = HuggingFaceEmbeddings(
        model_name="Bo8dady/finetuned4-College-embeddings",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )

    models = [ChatGoogleGenerativeAI(model=model_name, api_key=get_api_key(i)) for i in range(api_len)]
    return embedding_model, models
</file>

<file path="CollegeChatBot/splitData.py">
import pandas as pd
import os
# Load your file
base_dir = os.path.dirname(os.path.dirname(__file__))
chunk_path =   os.path.join(base_dir, "merged.csv")
df = pd.read_csv(chunk_path)

startAbstract = 0
endAbstract = 230
df.iloc[startAbstract:endAbstract].to_csv(f'abstracts.csv', index=False)
startEmail = 230
endEmail = 410
df.iloc[startEmail:endEmail].to_csv(f'emails.csv', index=False)
startRules = 410
endRules = 438
df.iloc[startRules:endRules].to_csv(f'rules.csv', index=False)
startFinal = 439
endFinals = 498
df.iloc[startFinal:endFinals].to_csv(f'finals.csv', index=False)
</file>

<file path="CollegeChatBot/utils.py">
import csv
import json

from langsmith import expect
from numpy.f2py.auxfuncs import throw_error


def upload_chunks(data_path):
    chunks = []
    with open(data_path, 'r', encoding='utf-8') as file:
        reader = csv.reader(file)
        for row in reader:
            if row and row[0]:
                chunks.append(row[0])
    return chunks
def parse_list(response) :
    response = response.strip()
    if response.startswith("```python"):
        response = response.replace("```python", "")
    elif response.startswith("```"):
        response = response.replace("```", "")
    response = response.strip('\n')
    try:
      jsonList  = json.loads(response)
      if isinstance(jsonList, list) and len(jsonList) == 4 and all(isinstance(x, int) for x in jsonList):
            return jsonList
      else :
          raise Exception(f"Invalid response from API: {response}")


    except json.JSONDecodeError as e:
        parse_error_message = f"JSON decoding error: {e}. Output was not valid JSON. and the output was {response}"
        print(parse_error_message)
        return [0,2,2,2]

    except Exception as e:
        parse_error_message = f"An unexpected error occurred during parsing: {e} and the output was {response}"
        print(parse_error_message)
        return [0,2,2,2]
</file>

<file path="generateDataset/datasetgenerationfinalexams.py">
# -*- coding: utf-8 -*-
"""dataSetGenerationFinalExams.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yR1N_53nqRQvRX--aSv4iaMCtZzcqSHM
"""

# !pip install langchain langchain-community langchain-google-genai huggingface-hub sentence-transformers python-dotenv ollama
# !pip install faiss-cpu

from langchain_community.llms import Ollama
from langchain_community.embeddings import HuggingFaceEmbeddings
import os
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain.schema import StrOutputParser
import time

models = [
    ChatGoogleGenerativeAI(model="gemini-2.0-flash-thinking-exp-01-21", api_key="your_api_key_1"),
    ChatGoogleGenerativeAI(model="gemini-2.0-flash-thinking-exp-01-21", api_key="your_api_key_2"),
    ChatGoogleGenerativeAI(model="gemini-2.0-flash-thinking-exp-01-21", api_key="your_api_key_3"),
    ChatGoogleGenerativeAI(model="gemini-2.0-flash-thinking-exp-01-21", api_key="your_api_key_4"),
    ChatGoogleGenerativeAI(model="gemini-2.0-flash-thinking-exp-01-21", api_key="your_api_key_5"),
]

embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-large-en-v1.5",  # or choose another BGE model variant
    model_kwargs={'device': 'cpu'}
)



with open("/content/FinalExams.txt", "r") as file:
    chunks = file.readlines()  # Reads all lines into a list
    chunks = [line.strip() for line in chunks]  # Removes trailing newline characters
chunks = [line for line in chunks if line]
print(chunks)

from langchain.vectorstores import FAISS
faiss_index = FAISS.from_texts(chunks, embeddings)

chunk_dict = {chunks[i] : i  for i in range(len(chunks)) }

from langchain.prompts import PromptTemplate

template = """
     You are an AI assistant that generates multiple variations of a question based on a given text context.
    - Your task is to rephrase the same question in at least **5 different ways** while maintaining its original meaning.
    - Include some variations that use common abbreviations (such as "algo" for "algorithm", "DS" for "data structures", "calc" for "calculus", etc.) if available .
    - Only output the questions.
    - Each question should be on a new line.
    - Do not include any explanations, headings, or additional text.

    ## Example Context:
    "The final exam for the Introduction to Algorithms course (CS101), offered by the Computer Science department, from 2024, is available at the following link:
    [https://drive.google.com/file/d/1xgNmx5X4Bj7cQNTb8ma4STZ-WgCHm05L/view?usp=sharing]."

    ### Expected Output:
    Where can I find the final exam link for the Introduction to Algorithms course from 2024?
    Could you share the link to the 2024 CS101 final exam?
    Is there a link available for the 2024 Algo course exam?
    How can I access the final exam for the Intro to Algorithms class from 2024?
    Can you provide the exam link for the CS101 Algo course from 2024?
    Where's the link for the 2024 Algorithms final?

    ## Now, generate multiple reworded questions for the following context:
    {context}
"""

prompt = PromptTemplate(
    template=template,
    input_variables=["context"]
)
print(prompt.format(context=chunks[0]))

idx = 0
def get_query (context) :
    while (True) :
      global idx
      idx+=1
      try :
        chain  = prompt |  models[idx%5] | StrOutputParser()
        response  = chain.invoke({"context": context})
        return response
      except Exception as e  :
        print(e)
        time.sleep(10)

result  =  get_query(chunks[17])
print(result)

import csv

csv_filename = "/content/dataset.csv"

with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:
    writer = csv.writer(file)
    writer.writerow(["Question", "chunk"])

def save_to_csv(data , context, filename="/content/dataset.csv"):
       x = [line.strip() for line in data.split('\n')]
       try :
             with open(filename, 'a', newline='', encoding='utf-8') as file:
                writer = csv.writer(file)
                for i in range(len(x)):
                    question = x[i]
                    writer.writerow([question, context])
             print(f"Data successfully saved to {filename}")
       except Exception as e :
            print("wrong in saving chunk ")

for i in range(len(chunks)):
  result  =  get_query(chunks[i])
  save_to_csv(result , chunks[i])

import pandas as pd

df = pd.read_csv("/content/dataset.csv")

RequiredNumber =  1000
def eval_MRR ():
    mrr = 0
    for i in range(RequiredNumber):
        RetreivedChunks = faiss_index.similarity_search(df["Question"][i] , k  = 5)
        for j in range(5):
            if (RetreivedChunks[j].page_content== df["chunk"][i]):
                mrr += 1/(j+1)
                break

    return mrr/RequiredNumber

ass =  eval_MRR()
print(ass)

RetreivedChunks = faiss_index.similarity_search("i need to study OR course final exams" , k  = 35)

for i in range(35):
  print(RetreivedChunks[i].page_content)

print(len(chunks))
</file>

<file path="generateDataset/doctorinfogeneration.py">
# -*- coding: utf-8 -*-
"""DoctorInfoGeneration.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TCzvuDz7RgnzWd3lC1NSYOnmRbfnY3RS
"""



from langchain_community.llms import Ollama
from langchain_community.embeddings import HuggingFaceEmbeddings
import os
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain.schema import StrOutputParser
import time

models = [
    ChatGoogleGenerativeAI(model="gemini-2.0-flash", api_key="AIzaSyAm6iLxJOey5xUM4eNN0UgaZbKytIlnMJg"),
    ChatGoogleGenerativeAI(model="gemini-2.0-flash", api_key="AIzaSyB0kYyqJ_HEaxRODzg8LaVz85KJs6oS7Dk"),
    ChatGoogleGenerativeAI(model="gemini-2.0-flash", api_key="AIzaSyCHigx1CQfHtl-66zuM1KSisHqfgXpxO48"),
    ChatGoogleGenerativeAI(model="gemini-2.0-flash", api_key="AIzaSyAVaS0j7g2o2ur665K_SldrwYoritqU5Mg"),
    ChatGoogleGenerativeAI(model="gemini-2.0-flash", api_key="AIzaSyBOeuFxdaQIwFqUCDR_ALr_MPLVTgF_7bI"),
]

embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-large-en-v1.5",  # or choose another BGE model variant
    model_kwargs={'device': 'cpu'}
)



with open("/content/DoctorInfo.txt", "r") as file:
    chunks = file.readlines()  # Reads all lines into a list
    chunks = [line.strip() for line in chunks]  # Removes trailing newline characters

print(chunks)



from langchain.vectorstores import FAISS
faiss_index = FAISS.from_texts(chunks, embeddings)

chunk_dict = {chunks[i] : i  for i in range(len(chunks)) }

from langchain.prompts import PromptTemplate

template = """
  You are an AI assistant that generates questions based on given text contexts.
    Your task is to generate exactly **two questions** for each provided context.

    - Only output the questions.
    - Each question should be on a new line.
    - Do not include any explanations, headings, or additional text.

    ## Example Context:
    "Dr. Abeer Mahmoud is part of the CS department and can be reached at abeer.mahmoud@cis.asu.edu.eg."

    ### Expected Output:
    Which department does Dr. Abeer Mahmoud belong to?
    What is Dr. Abeer Mahmoud's email address?

    ## Now, generate questions for the following contexts:
    {context}
"""

prompt = PromptTemplate(
    template=template,
    input_variables=["context"]
)
print(prompt.format(context=chunks[0]))

idx = 0
def get_query (context) :
    while (True) :
      global idx
      idx+=1
      try :
        chain  = prompt |  models[idx%5] | StrOutputParser()
        response  = chain.invoke({"context": context})
        return response
      except Exception as e  :
        print(e)
        time.sleep(10)

result  =  get_query(chunks[0])
print(result)

import csv

csv_filename = "/content/dataset.csv"

with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:
    writer = csv.writer(file)
    writer.writerow(["Question", "chunk Index"])

def save_to_csv(data , context_index, filename="/content/dataset.csv"):
       x = [line.strip() for line in data.split('\n')]
       try :
             with open(filename, 'a', newline='', encoding='utf-8') as file:
                writer = csv.writer(file)
                for i in range(len(x)):
                    question = x[i]
                    writer.writerow([question, context_index])
             print(f"Data successfully saved to {filename}")
       except Exception as e :
            print("wrong in saving chunk ")

for i in range(len(chunks)):
  result  =  get_query(chunks[i])
  save_to_csv(result , i)

import pandas as pd

df = pd.read_csv("/content/dataset.csv")

RequiredNumber =  186 * 2
def eval_MRR ():
    mrr = 0
    for i in range(RequiredNumber):
        RetreivedChunks = faiss_index.similarity_search(df["Question"][i] , k  = 5)
        for j in range(5):
            if (chunk_dict[RetreivedChunks[j].page_content] == df["chunk Index"][i]):
                mrr += 1/(j+1)
                break

    return mrr/RequiredNumber

ass =  eval_MRR()
print(ass)

RetreivedChunks = faiss_index.similarity_search("what is the email address of Dr. mohamed essam  ? " , k  = 5)

print(RetreivedChunks)
</file>

<file path="generateDataset/generateanouncementdataset.py">
# -*- coding: utf-8 -*-
"""GenerateAnouncementDataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1STWoVDMy4Erec84WD2HPh-KNf-D-d-BF
"""

# !pip install langchain langchain-community langchain-google-genai huggingface-hub sentence-transformers python-dotenv ollama faiss-cpu

from langchain_community.llms import Ollama
from langchain_community.embeddings import HuggingFaceEmbeddings
import os
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain.schema import StrOutputParser
import time

models = [
    ChatGoogleGenerativeAI(model="gemini-2.0-flash-thinking-exp-01-21", api_key="your_api_key_1"),
    ChatGoogleGenerativeAI(model="gemini-2.0-flash-thinking-exp-01-21", api_key="your_api_key_2"),
    ChatGoogleGenerativeAI(model="gemini-2.0-flash-thinking-exp-01-21", api_key="your_api_key_3"),
    ChatGoogleGenerativeAI(model="gemini-2.0-flash-thinking-exp-01-21", api_key="your_api_key_4"),
    ChatGoogleGenerativeAI(model="gemini-2.0-flash-thinking-exp-01-21", api_key="your_api_key_5"),
]

embeddings = HuggingFaceEmbeddings(
    model_name="Bo8dady/finetuned-College-embeddings",  # or choose another BGE model variant
    model_kwargs={'device': 'cpu'}
)

with open("/content/Announcemnt_data.txt", "r") as file:
    chunks = file.readlines()  # Reads all lines into a list
    chunks = [line.strip() for line in chunks]  # Removes trailing newline characters
chunks = [line for line in chunks if line]
print(chunks)

from langchain.vectorstores import FAISS
faiss_index = FAISS.from_texts(chunks, embeddings)

from langchain.prompts import PromptTemplate

template = """
     You are an advanced AI tasked with generating multiple structured questions from educational announcements.

Given the following announcement: {context}

Generate 4-5 distinct question variations that comprehensively explore the announcement's key details. Follow these guidelines:
1. Use different interrogative words (When, What, Where, Who, How)
2. Vary question structures and phrasings
3. Capture the core information from the original text
4. Ensure questions are grammatically correct and meaningful
5. Provide unique perspectives on the announcement

Output Requirements:
- Generate exactly 4-7 questions
- Place each question on a separate line
- Avoid redundant or overly similar questions

Questions:
"""

prompt = PromptTemplate(
    template=template,
    input_variables=["context"]
)
print(prompt.format(context=chunks[0]))

idx = 0
def get_query (context) :
    while (True) :
      global idx
      idx+=1
      try :
        chain  = prompt |  models[idx%len(models)] | StrOutputParser()
        response  = chain.invoke({"context": context})
        return response
      except Exception as e  :
        print(e)
        time.sleep(10)

result  =  get_query(chunks[17])
print(result)

import csv

csv_filename = "/content/dataset.csv"

with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:
    writer = csv.writer(file)
    writer.writerow(["Question", "chunk"])

def save_to_csv(data , context, filename="/content/dataset.csv"):
       x = [line.strip() for line in data.split('\n')]
       try :
             with open(filename, 'a', newline='', encoding='utf-8') as file:
                writer = csv.writer(file)
                for i in range(len(x)):
                    question = x[i]
                    writer.writerow([question, context])
             print(f"Data successfully saved to {filename}")
       except Exception as e :
            print("wrong in saving chunk ")

for i in range(len(chunks)):
  result  =  get_query(chunks[i])
  save_to_csv(result , chunks[i])
</file>

<file path="generateDataset/rulesgeneration_dataset.py">
# -*- coding: utf-8 -*-
"""RulesGeneration Dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EFGmtHrKQG-ZExngsVB5xm5_fge1rdF3
"""

#pip install langchain langchain-community langchain-google-genai huggingface-hub sentence-transformers python-dotenv ollama

from langchain_community.llms import Ollama
from langchain_community.embeddings import HuggingFaceEmbeddings
import os
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain.schema import StrOutputParser
import time

import json
chunks =  []
file_path = "/content/rules.json"

try:
    with open(file_path, 'r') as f:
        data = json.load(f)
    for item in data:
        chunk_text = item["chunk"]
        chunks.append(chunk_text)
        print("Chunk Content:")
        print(chunk_text)
        print("-" * 50)

except FileNotFoundError:
    print(f"Error: File not found at path: {file_path}")
except json.JSONDecodeError:
    print(f"Error: Could not decode JSON from file: {file_path}. Please ensure it is valid JSON.")
except Exception as e:
    print(f"An unexpected error occurred: {e}")

models = [
    ChatGoogleGenerativeAI(model="gemini-2.0-flash-thinking-exp-01-21", api_key="your_api_key_1"),
    ChatGoogleGenerativeAI(model="gemini-2.0-flash-thinking-exp-01-21", api_key="your_api_key_2"),
    ChatGoogleGenerativeAI(model="gemini-2.0-flash-thinking-exp-01-21", api_key="your_api_key_3"),
    ChatGoogleGenerativeAI(model="gemini-2.0-flash-thinking-exp-01-21", api_key="your_api_key_4"),
    ChatGoogleGenerativeAI(model="gemini-2.0-flash-thinking-exp-01-21", api_key="your_api_key_5"),
]

from  langchain.prompts import PromptTemplate

refine_template = PromptTemplate(
    input_variables=["context"],
    template='''Extract the topic of the given chunk, summarize it concisely, and then provide the chunk itself. Output only these three elements without extra text.

Example:
Chunk: "College Departments\nThe college consists of the following scientific departments:\n1. Department of Computer Science\n2. Department of Information Systems\n3. Department of Basic Sciences\n4. Department of Computer Systems\n5. Department of Scientific Calculations\nThe college may establish other departments in the college in accordance with the provisions of the Universities Organization Law."

Expected Output:
Topic: College Departments
Summary: The college has five scientific departments and may establish more as per regulations.
Chunk: "College Departments\nThe college consists of the following scientific departments:\n1. Department of Computer Science\n2. Department of Information Systems\n3. Department of Basic Sciences\n4. Department of Computer Systems\n5. Department of Scientific Calculations\nThe college may establish other departments in the college in accordance with the provisions of the Universities Organization Law."

Now, process the following chunk:
Chunk: "{context}"'''
)

idx = 0
def get_query ( prompt, context) :
    while (True) :
      global idx
      idx+=1
      try :
        chain  = prompt |  models[idx%5] | StrOutputParser()
        response  = chain.invoke({"context": context})
        print("ok")
        return response
      except Exception as e  :
        print(e)
        time.sleep(10)

print(get_query(refine_template ,chunks[0]))

refined_chunks = []
for  chunk in chunks :
  refined_chunks.append(get_query(chunk))

print(len(refined_chunks))

question_prompt = PromptTemplate(
    input_variables=["context"],
    template='''Generate a list of ** from 5 to 12** relevant questions a student might ask based on the given chunk. Ensure a mix of direct and open-ended questions that there answers are exist in the chunk. Output only the questions in the following format:

Example:
Chunk: "The Computer Science department focuses on programming, algorithms, and artificial intelligence."
Output:
- What topics are covered in the Computer Science department?
- Importance of artificial intelligence in the curriculum?
- How does programming relate to artificial intelligence?
- Career opportunities for Computer Science graduates?
- Key skills needed for success in Computer Science?
- How do algorithms help in software development?
- Specializations available within Computer Science?

Now, generate **5 to 12** questions for the following chunk:
Chunk: "{context}"
Output:'''
)

print(get_query(question_prompt ,refined_chunks[0]))

import csv

csv_file = "/content/rulesDataset.csv"

with open(csv_file , mode = 'w'  , newline='' ,encoding='utf-8') as file :
  writer  =  csv.writer(file)
  writer.writerow(["question" , "chunk"])

def add_to_csv(data , chunk ) :
  questions = [line for line in data.split('\n')]
  try :
    with open(csv_file , mode = 'a' , newline='' , encoding='utf-8') as file :
      writer  =  csv.writer(file)
      for question in questions :
        writer.writerow([question , chunk])
        print("write ok")
  except Exception as e :
    print(e)

for chunk in refined_chunks :
  add_to_csv(get_query(question_prompt ,chunk) , chunk)
</file>

<file path="ModularChatBot.py">
import os
from dotenv import load_dotenv
from langchain_community.embeddings import JinaEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.schema import StrOutputParser
from langchain_core.runnables import RunnableLambda
from flask import Flask, request, jsonify
from langchain_community.embeddings import HuggingFaceEmbeddings
import csv
app = Flask(__name__)
api_len = 10 
def setup_environment():
    """Set up the environment by loading API keys."""
    load_dotenv()
def initialize_models(model_name):
    """Initialize embeddings and language model."""
    EMBEDDING_MODEL_NAME = "Bo8dady/finetuned4-College-embeddings"  

    embeddings = HuggingFaceEmbeddings(
                 model_name=EMBEDDING_MODEL_NAME,
    model_kwargs={'device': 'cpu'},  
    encode_kwargs={'normalize_embeddings': True}
)

   
    models = []
    for i in range(0,api_len) :
        models.append(ChatGoogleGenerativeAI(model=model_name, api_key= os.getenv(f"G{i}")))
    return embeddings, models    
def upload_chunks(data_path):
    chunks =[]
    with open(data_path , 'r' , encoding ='utf-8') as file:
        reader =  csv.reader(file)
        for i, row in enumerate(reader):
                if row: 
                    if len(row) > 0:
                        chunks.append(row[0])
    return chunks
def manage_faiss_index(chunks, embeddings, index_folder):
    """Load or create a FAISS index based on the PDF's modification time."""
    index_file = os.path.join(index_folder, "index.faiss")
    if os.path.exists(index_file):
        try:
            faiss_index = FAISS.load_local(index_folder, embeddings, allow_dangerous_deserialization=True)
            print("Loaded existing FAISS index.")
        except Exception as e:
            print(f"Error loading FAISS index: {e}. Creating a new one.")
            faiss_index = FAISS.from_texts(chunks, embeddings)
            faiss_index.save_local(index_folder)
    else:
        print("FAISS index not found. Creating a new one.")
        faiss_index = FAISS.from_texts(chunks, embeddings)
        faiss_index.save_local(index_folder) 
    return faiss_index
def setup_retriever(faiss_index , num_retrievs=10):
    """Set up the retriever using the FAISS index."""
    return faiss_index.as_retriever(search_kwargs={"k": num_retrievs})

def initialize_prompt_templates():
    """Define and return prompt templates for question answering and refinement."""
    qa_template = ChatPromptTemplate.from_messages([
        ("system", """Answer the question based on the context below and make the answer very organized and have markup . 
           Context: {context}
           Question: {question}
        """),
        ("human", "question: {question}\n context: {context}"),
    ])

    refine_template = ChatPromptTemplate.from_messages([
        ("system", "Rephrase the student's question for clarity while preserving its meaning and if there something mispealing corect it. and make sure that the outpute is in english and  only one question? "),
        ("human", "Question to refine: {question}")
    ])

    return qa_template, refine_template
def get_chains(retriever  , qa_template, refine_template , model , idx):
    chain_question_refine = refine_template | model[idx] | StrOutputParser()
    chain_retrieve_docs = retriever | RunnableLambda(lambda x: "\n\n".join([doc.page_content for doc in x]))
    chain_answer_question = qa_template | model[idx] | StrOutputParser()
    return chain_question_refine, chain_retrieve_docs, chain_answer_question
def get_answer(question):
    """Generate an answer to a question using the defined chains."""
    idx =  0 
    while True :
        idx+=1 
        idx%= api_len
        try:
            chain_question_refine, chain_retrieve_docs, chain_answer_question = get_chains(retriever, qa_template, refine_template, models, idx)
            refined_question = chain_question_refine.invoke({"question": question})
            context = chain_retrieve_docs.invoke(refined_question)
            print(refined_question)
            answer = chain_answer_question.invoke({"question": refined_question, "context": context})  
            return answer
        except Exception as e:
            print(f"Error generating answer: {e}")
            return None
@app.route('/ModularChatBot', methods=['POST'])
def chatbot():
    # Get the question from the JSON payload
    data = request.get_json()
    question = data.get('question', '')
    print("Question received:", question)
    # Process the question (this is where your logic goes)
    answer = get_answer(question)
    # Return the answer as JSON
    return jsonify({'answer': answer})         
if __name__ == "__main__":
    data_path = "chunks.csv"
    index_folder = "faiss_index"
    setup_environment() ; 
    embeddings, models = initialize_models("gemini-2.0-flash")
    chunks = upload_chunks(data_path)
    faiss_index = manage_faiss_index(chunks, embeddings, index_folder)
    qa_template, refine_template = initialize_prompt_templates()
    retriever = setup_retriever(faiss_index)
    app.run(host='0.0.0.0', port=3000)
</file>

<file path="requirements.txt">
python-dotenv
langchain-community
langchain-google-genai
langchain-core
langchain
faiss-cpu
pypdf
flask
google-generativeai
fastapi
uvicorn
pydantic
pandas
</file>

<file path="CollegeChatBot/app.py">
from fastapi import FastAPI
from langchain.memory import ConversationBufferWindowMemory
from pydantic import BaseModel
from config import setup_environment
from models import initialize_models
from utils import upload_chunks
from faiss_handler import manage_faiss_index
from chatbot import get_answer
import os
import uvicorn
import uuid
from langchain_community.chat_message_histories import RedisChatMessageHistory


app = FastAPI()

# Global variables
retriever = None
models = None
REDIS_URL  =None
memory = ConversationBufferWindowMemory(k=4, memory_key="chat_history", return_messages=True)
faiss_indexes = {}

# Define the request model
class QuestionInput(BaseModel):
    question: str


@app.on_event("startup")
async def startup_event():
    global retriever, models , faiss_indexes ,  REDIS_URL
    print("üîß Initializing environment and models...")
    setup_environment()
    REDIS_URL = os.getenv("REDIS_URL")
    embeddings, models = initialize_models("gemini-2.5-flash-preview-05-20")
    data_sources_config = {
        "abstracts": {"file": "abstracts.csv", "index_folder": "faiss_abstracts"},
        "emails": {"file": "emails.csv", "index_folder": "faiss_emails"},
        "finals": {"file": "finals.csv", "index_folder": "faiss_finals"},
        "rules": {"file": "rules.csv", "index_folder": "faiss_rules"},
    }
    base_dir = os.path.dirname(os.path.dirname(__file__))
    for db_name, config in data_sources_config.items():
        data_path = os.path.join(base_dir, "data", config["file"])
        index_folder_path = os.path.join(base_dir, config["index_folder"])
        chunks = upload_chunks(data_path)
        faiss_index = manage_faiss_index(chunks, embeddings, index_folder_path)
        faiss_indexes[db_name] = faiss_index


    chunk_path = os.path.join(base_dir, "merged.csv")
    chunks = upload_chunks(chunk_path)
    try:
        from redis import Redis
        r = Redis.from_url(REDIS_URL)
        await r.ping()
        print("‚úÖ Successfully connected to Redis.")
    except Exception as e:
        print(f"‚ùå Could not connect to Redis at {REDIS_URL}: {e}")
        exit(1)

    faiss_index = manage_faiss_index(chunks, embeddings, "faiss_index")
    retriever = faiss_index.as_retriever(search_kwargs={"k": 5})
    print("‚úÖ Startup complete.")


@app.get("/ModularChatBot")
async def chatbot_api(payload: QuestionInput):
    global retriever, models , memory
    question = payload.question
    session_id = payload.session_id

    if not session_id:
        session_id = uuid.uuid4()
        print(f" NEW SESSION ID: {session_id}")

    message_history = RedisChatMessageHistory(
        session_id=session_id,
        url = os.getenv("REDIS_URL")
    )

    current_session_memory = ConversationBufferWindowMemory(
        k=4,
        memory_key="chat_history",
        chat_memory=message_history,
        return_messages=True
    )
    print("Received:", question)
    answer = await get_answer(question, models, faiss_indexes , memory)
    memory.save_context({"question": question}, {"answer": answer})
    print(memory)
    return {"session_id": session_id, "answer": answer}


if __name__ == "__main__":
    # Run using uvicorn from this file directly
    uvicorn.run("app:app", host="127.0.0.1", port=8000, reload=True)
</file>

<file path="CollegeChatBot/chains_and_prompts.py">
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnableLambda, RunnableParallel, RunnablePassthrough
from langchain.schema import StrOutputParser
from operator import itemgetter


class Templates:
    """Define and return prompt templates for question answering and refinement."""
    qa_template = ChatPromptTemplate.from_messages([
        ("system", """ 
            You will be given a question and context, and you must understand the context and the question to answer in natural language.
           If the conversation history is available, use it to understand follow-up questions.  
        """),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "question: {question}\n context: {context}"),
    ])

    refine_template = ChatPromptTemplate.from_messages([
        ("system",
         "Rephrase the student's question for clarity while preserving its meaning and if there something mispealing correct it. Make sure the output is in English and only one question."),
        ("human", "Question to refine: {question}")
    ])

    cluster_template = ChatPromptTemplate.from_messages([
        ("system", """ You are an intelligent query analysis assistant for a college information system. Your primary task is to determine the optimal number of documents to retrieve from each of four distinct data sources to best answer a student's question.
        The four data sources are, in this specific order:
        1. Abstracts: Contains summaries and details of student graduation projects and research.
        2. Emails: Contains contact information (email addresses, departments) for college doctors and staff.
        3. Finals: Contains links to past final exam papers for various courses.
        4. Rules: Contains official college regulations, academic rules, course prerequisites, and program structures.
        You will be given:
        1. The User's Question.
        2. the chat history . 
        3. A Sample Snippet from each of the four data sources to help you understand the type of information each contains.
        Your goal is to output a Python list of 4 integers. Each integer represents the estimated number of relevant documents that should be retrieved from the corresponding data source to answer the question. The order in the list MUST match the order of the data sources listed above:‚Ä®[number_for_Abstracts, number_for_Emails, number_for_Finals, number_for_Rules]
        Guidelines for determining the number of documents:
        * Relevance is Key: If a data source is highly relevant and likely to contain the core answer, suggest 1-3 documents. For broader queries requiring exploration (e.g., "find projects about AI"), you might suggest up to 10 from 'Abstracts'.
        * Specificity:
            * 
            * If the question asks for a single piece of specific information (e.g., "email of Dr. Smith"), 1 or 2  document from the 'Emails' source is likely sufficient.
            * If the question asks about multiple specific items (e.g., "final exams for CS101 for 2020, 2021, and 2022"), you might suggest 1-3 documents from 'Finals', assuming each year/course might be a separate document or closely related ones.
        * Irrelevance: If a data source is clearly irrelevant to the question, use 0 for that source.
        * Conciseness: Prefer fewer documents if the answer is likely to be found in a small number of highly relevant chunks. Avoid requesting many documents unnecessarily.
        * No Information: If you believe none of the data sources can answer the question, you can output [0, 0, 0, 0].
        Example Scenarios:
        * Question: "What is Dr. Eman Hamdi's email address?"
            * 
            * Expected Output Reasoning: This is a direct request for an email. 'Emails' is the only relevant source.
            * Expected Output: [0, 1, 0, 0]
        * Question: "Can you show me some graduation project abstracts on machine learning?"
            * 
            * Expected Output Reasoning: This is about project abstracts. 'Abstracts' is primary. A few examples would be good.
            * Expected Output: [3, 0, 0, 0] (Suggesting 3 abstracts)
        * Question: "Where can I find the final exam for Algorithms from 2021 and what are the prerequisites for the advanced algorithms course?"
            * 
            * Expected Output Reasoning: Needs 'Finals' for the exam link and 'Rules' for course prerequisites.
            * Expected Output: [0, 0, 1, 1] (or [0, 0, 1, 2] if prerequisites are complex and might span multiple rule chunks)
        * Question: "What are the college's rules regarding plagiarism and what's the email for the head of the CS department?"
            * 
            * Expected Output Reasoning: Needs 'Rules' for plagiarism and 'Emails' for the department head.
            * Expected Output: [0, 1, 0, 1] 
        """),
        MessagesPlaceholder(variable_name="chat_history")
        ,
        ("human", """
                Now, analyze the following:
        User's Question:
              {question}
            
        Data Source Samples:
        1. Sample from 'Abstracts' Data Source:
              **Faculty Information**
        
        Faculty of Computer Science and Information
        Scientific Computing Department
        
        **Abstract**
        
        During the last few decades, with the rise of Youtube, Amazon, Netflix and many other such web services, recommender systems have taken more and more place in our lives. From e-commerce (suggest to buyers articles that could interest them) to online advertisement (suggest to users the right contents, matching their preferences), recommender systems are today unavoidable in our daily online journeys.
        
        **Recommender Systems as Algorithms**
        
        In a very general way, recommender systems are algorithms aimed at suggesting relevant items to users (items being movies to watch, text to read, products to buy or anything else depending on industries).
            
        2. Sample from 'Emails' Data Source:
              Dr. Eman Hamdi is part of the Unknown department and can be reached at emanhamdi@cis.asu.edu.eg.
        3. Sample from 'Finals' Data Source:
              The final exam for Algorithms Analysis & Design course, offered by the scientific computing department, from 2021, is available at the following link: [https://drive.google.com/file/d/1mT21jrptv4w2IdFd5G-oVxI_6lR_SrS9/view
        The final exam for Algorithms Analysis & Design course, offered by the scientific computing department, from 2020, is available at the following link: [https://drive.google.com/file/d/1W_X01ASI0yyo6fCG4SwZYlvoE-Sh_C0g/view?usp=drive_link
        4. Sample from 'Rules' Data Source:
              Topic: Scientific Computing Fourth Level Courses
        Summary: Outlines the courses, credit hours, and prerequisites for the seventh and eighth semesters of the fourth level Scientific Computing program.
        Chunk: ""Fourth Level Courses For Scientific Computing (SC4) 
        Seventh Semester  
        ‚Ä¢ SCO422: Computational Geometry (3 Credit Hours) ‚Äì Prerequisite: SCO311 (Computer 
        Graphics) 
        ‚Ä¢ SCO411: Neural Networks & Deep Learning (3 Credit Hours) ‚Äì Prerequisite: BSC225 (Linear 
        Algebra) 
        ‚Ä¢ SCO421: Computer Vision (3 Credit Hours) ‚Äì Prerequisite: CIS243 (Artificial Intelligence) 
        ‚Ä¢ Total Credit Hours: 18 
        Eighth Semester 
        ‚Ä¢ CSY410: Computer and Network Security (3 Credit Hours) ‚Äì Prerequisite: CIS365
            
        Your Output (Python list of 4 integers only):
                """)
    ])


def get_chains(models, idx, memory):
    """Build LangChain pipelines for question refinement, retrieval, and answering."""
    chain_question_refine = Templates.refine_template | models[idx] | StrOutputParser()
    chain_answer_question = (
            RunnablePassthrough.assign(
                chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter("chat_history")
            )
            | Templates.qa_template | models[idx] | StrOutputParser()
    )
    chain_cluster_question = (
            RunnablePassthrough.assign(
                chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter("chat_history")
            )
            | Templates.cluster_template | models[idx] | StrOutputParser()
    )
    return chain_question_refine, chain_answer_question, chain_cluster_question
</file>

<file path="CollegeChatBot/chatbot.py">
from langchain.memory import ConversationBufferWindowMemory

from chains_and_prompts import get_chains
from utils import *
from langchain_core.runnables import RunnableLambda, RunnableParallel
from chains_and_prompts import Templates
import asyncio

api_len = 10


async def get_answer(question, models, faiss_indexes , memory):
    idx = 0

    try:
        idx += 1
        idx %= api_len

        _, answer_chain, cluster = get_chains(models, idx, memory)
        output = await cluster.ainvoke({"question": question})
        output = parse_list(output)
        print(output)
        db_names = ["abstracts", "emails", "finals", "rules"]
        retrieved_docs = []
        retrieved_tasks = []
        for i, counter in enumerate(output):
            if counter > 0:
                faiss_index = faiss_indexes.get(db_names[i])

                if faiss_index:
                    dynamic_retriever = faiss_index.as_retriever(search_kwargs={"k": counter})
                    retrieved_tasks.append(dynamic_retriever.ainvoke(question))
                else:
                    print(f"  Warning: FAISS index for '{db_names[i]}' not found in global 'faiss_indexes'.")

        retrieved_docs_list = await asyncio.gather(*retrieved_tasks)
        for docs_from_db in retrieved_docs_list:
            for doc in docs_from_db:
                retrieved_docs.append(doc.page_content)
        context = "\n\n".join(retrieved_docs)
        try:
            idx += 1
            idx %= len(models)
            answer = await answer_chain.ainvoke({"question": question, "context": context})
            return answer
        except Exception as e:
            print(f"Error during final answer generation (LLM index {idx}): {e}")
            # Try next LLM or return an error message
            return "I apologize, but I encountered an error while generating the answer. Please try again later."




    except Exception as e:
        print(f"Error at index {idx}: {e}")
</file>

</files>
